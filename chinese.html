<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Andrew Luo</title>

    <meta name="author" content="Andrew Luo">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr style="padding:0px">
                    <td style="padding:2.5%;width:65%;vertical-align:middle">
                        <p class="name" style="text-align: center;">
                            Andrew Luo
                        </p>
                        <p> 我是<a href="https://www.hku.hk/">香港大学</a>助理教授，由香港大学<a href="https://datascience.hku.hk/">数据科学研究院</a>和<a href="https://psychology.hku.hk/people/andrew-luo/">心理学系</a>于 2024 年 10 月合聘。
                        </p>
                        <p>
                            我在2024从卡内基梅隆大学 (CMU) 获得了<a href="https://www.ml.cmu.edu/academics/joint-ml-phd.html#:~:text=PhD%20in%20Neural%20Computation%20%26%20Machine%20Learning"><u>机器学习</u>和<u>计算神经科学</u>联合博士学位</a>。在那里我与 <a href="https://www.cmu.edu/dietrich/psychology/directory/core-training-faculty/tarr-michael.html">Michael
                            Tarr</a> and <a
                                href="https://www.cs.cmu.edu/~lwehbe/">Leila Wehbe</a> 一起工作。在此之前，我于 2019 年获得了麻省理工学院 (MIT) 计算机科学本科学位。我也从 CMU 获得了机器学习硕士学位。 </p><p>我的工作重点是理解<u>视觉感知背后的计算原理</u>，以及这些原理如何能帮助我们<u>改善并设计更好的AI生成模型</u>。最终，我的目标是弥合人类和机器视觉之间的差距，从而加深对人类认知的理解 & 达到人工智能的进步。


                        </p>
<!--                        <p style="font-size: 18px;color: seagreen">2024 年末和2025 夏天，我正在招有<u>机器视觉</u>, <u>AI for Neuroscience (NeuroAI)</u>, 和<u>图像生成模型</u>背景的博士生加入我的研究团队(加入港大)。也欢迎博士生，硕士生和本科生远程合作或者远程RA。请发送电子邮件至 aluo@hku.hk，附上简历和自我介绍。</p>-->
<!--                        <p style="font-size: 18px;color: seagreen">2024 年 11 月 25 号更新：我已经完成了2025秋的IDS博士生录取。仍然在招收心理系的博士生。</p>-->
                        <p style="font-size: 18px;color: seagreen">我已经完成了2025秋的博士生录取。仍然欢迎博士生，硕士生和本科生远程合作或者RA。</p>

                        <p style="text-align:center">
                            <a href="mailto:luo_andrew@outlook.com">Email</a> &nbsp;/&nbsp;
                            <a href="mailto:aluo@hku.hk">HKU Email</a> &nbsp;/&nbsp;
                            <a href="https://scholar.google.com/citations?hl=en&user=bWYvvkUAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp;/&nbsp;
                            <!--                  <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp;-->
                            <a href="https://github.com/aluo-x">Github</a>&nbsp;/&nbsp;
                            <a href="./wechat.html">WeChat</a> /
                            <a href="./index.html">English</a>
                        </p>
                    </td>
                    <td style="padding:2.5%;width:40%;max-width:40%">
                        <img style="width:100%;max-width:100%" alt="profile photo"
                                                            src="images/profile_2.jpg" class="hoverZoomLink">
                    </td>
                </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>研究</h2>
                        <p>
                            我想理解人类的感知机理，并构建更好的生成模型和具有类人推理能力的机器。
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="./images/ph_reg_teaser.jpg" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2505.21501" id="phreg">
                            <span class="papertitle">Vision Transformers with Self-Distilled Registers</span>
                        </a>
                        <br>
                        <a href="https://openreview.net/profile?id=~Yinjie_Chen3">Yinjie Chen</a>, <a href="https://openreview.net/profile?id=~Zipeng_Yan1">Zipeng Yan</a>, <a href="http://chongzhou96.github.io/">Chong Zhou</a>, <a href="http://daibo.info/">Bo Dai</a>, <strong>Andrew F. Luo</strong>
                        <br>

                        <span> <em>in submission</em> </span>
                        <br>
                        <a href="https://arxiv.org/abs/2505.21501">arxiv
                            page</a> /
                            <a href="data/PH_reg_bib.html">bibtex</a>
                        <p></p>
                        <p>We show that artifacts can be removed from pre-trained ViTs without any labeled data by introducing registers in post-training. Our method uses a model combined with test-time augmentation to distill itself, leading to significant improvements in open-vocabulary segmentation and dense prediction tasks.</p>
                    </td>
                </tr>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="./images/Brain_CoRL_teaser.jpg" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2505.15813" id="braincorl">
                            <span class="papertitle">Meta-Learning an In-Context Transformer Model of Human Higher Visual Cortex
</span>
                        </a>
                        <br>
                        <a href="https://scholar.google.com/citations?hl=en&user=8fm4uE8AAAAJ">Muquan Yu</a>, <a href="https://openreview.net/profile?id=~Mu_Nan2">Mu Nan</a>, <a href="https://hosseinadeli.github.io/">Hossein Adeli</a>, <a href="https://jacob-prince.github.io/">Jacob S. Prince</a>, <a href="https://psych.uw.edu/people/9200">John A. Pyles</a>, <a href="https://www.cs.cmu.edu/~lwehbe/">Leila Wehbe</a>, <a href="https://www.maggie-henderson.com/">Margaret M. Henderson</a>, <a href="https://sites.google.com/andrew.cmu.edu/tarrlab/people/michael-j-tarr">Michael J. Tarr</a>, <strong>Andrew F. Luo</strong>
                        <br>

                        <span> <em>in submission</em> </span>
                        <br>
                        <a href="https://arxiv.org/abs/2505.15813">arxiv
                            page</a> /
                            <a href="data/Braincorl_bib.html">bibtex</a>
                        <p></p>
                        <p>We show how to construct higher visual cortex encoders that can generalize across subjects, scanners, voxel sizes, protocols, and images without any additional finetuning by using meta-learning across subjects and in-context learning across stimuli.</p>
                    </td>
                </tr>


                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="./images/brainnerds_compressed.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2406.02659" id="brainnerd">
                            <span class="papertitle">Reanimating Images using Neural Representations of Dynamic Stimuli</span>
                        </a>
                        <br>
                        <a href="https://jacobyeung.org/">Jacob Yeung</a>, <strong>Andrew F. Luo</strong>, <a href="https://gabesarch.me/">Gabriel Sarch</a>, <a href="https://www.maggie-henderson.com/">Margaret M. Henderson</a>, <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a>, <a href="https://sites.google.com/andrew.cmu.edu/tarrlab/people/michael-j-tarr">Michael J. Tarr</a>
                        <br>

                        <span style="color:orangered;"> <em>CVPR 2025</em> oral</span>
                        <br>
                        <a href="https://arxiv.org/abs/2406.02659">arxiv
                            page</a> /
                            <a href="data/BrainNerd_bib.html">bibtex</a>
                        <p></p>
                        <p>We propose image-conditioned decoding of perceived motion from fMRI data, we show that this can be used to animate images with an video diffusion model.</p>
                    </td>
                </tr>


                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="./images/brainsail_teaser.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2410.05266" id="brainsail">
                            <span class="papertitle">Brain Mapping with Dense Features: Grounding Cortical Semantic Selectivity in Natural Images With Vision Transformers</span>
                        </a>
                        <br>
                        <strong>Andrew F. Luo</strong>, Jacob Yeung, Rushikesh Zawar, Shaurya Dewan, <a href="https://www.maggie-henderson.com/">Margaret M. Henderson</a>, <a href="https://www.cs.cmu.edu/~lwehbe/">Leila Wehbe*</a>, <a href="https://sites.google.com/andrew.cmu.edu/tarrlab/people/michael-j-tarr">Michael J. Tarr*</a>
                        <br>
                        * Co-corresponding authors
                        <br>
                        <em>ICLR 2025</em>
                        <br>
                        <a href="https://arxiv.org/abs/2410.05266">arxiv
                            page</a> /
                            <a href="data/BrainSAIL_bib.html">bibtex</a>
                        <p></p>
                        <p>We propose an efficient gradient-free distillation module capable of extraction high quality dense CLIP embeddings, and utilize these embeddings to understand semantic selectivity in the visual cortex.</p>
                    </td>
                </tr>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="./images/audionav.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2407.11333" id="audionav">
                            <span class="papertitle">Disentangled Acoustic Fields For Multimodal Physical Scene Understanding</span>
                        </a>
                        <br>
                        Jie Yin, <strong>Andrew F. Luo</strong>, <a href="https://yilundu.github.io/">Yilun Du</a>, <a href="https://www.merl.com/people/cherian"> Anoop Cherian</a>, <a href="https://www.merl.com/people/tmarks">Tim K Marks</a>, <a href="https://www.jonathanleroux.org/">Jonathan Le Roux</a>, <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>
                        <br>
                        <em>IROS 2024</em>
                        <br>
                        <a href="https://arxiv.org/abs/2407.11333">arxiv
                            page</a> /
                            <a href="data/AudioNav_bib.html">bibtex</a>
                        <p></p>
                        <p>We investigate the problem of visual-acoustic navigation conditioned on a continuous acoustic field representation of audio.</p>
                    </td>
                </tr>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="./images/diffusionpid.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2406.05191" id="diffusionpid">
                            <span class="papertitle">DiffusionPID: Interpreting Diffusion via Partial Information Decomposition</span>
                        </a>
                        <br>
                        Shaurya Dewan, Rushikesh Zawar, Prakanshul Saxena, Yingshan Chang, <strong>Andrew F. Luo</strong>, <a href="https://talkingtorobots.com/yonatanbisk.html">Yonatan Bisk</a>
                        <br>
                        <em>NeurIPS 2024</em>
                        <br>
                        <a href="https://arxiv.org/abs/2406.05191">arxiv
                            page</a> /
                            <a href="data/diffusionpid_bib.html">bibtex</a>
                        <p></p>
                        <p>We leverage ideas from information theory to understand the contributions of individual text tokens and their interactions when generating images.</p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="./images/BrainSCUBA_teaser.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2310.04420" id="brainscuba">
                            <span class="papertitle">BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity</span>
                        </a>
                        <br>
                        <strong>Andrew F. Luo</strong>, <a href="https://www.maggie-henderson.com/">Margaret M. Henderson</a>, <a href="https://sites.google.com/andrew.cmu.edu/tarrlab/people/michael-j-tarr">Michael J. Tarr</a>, <a href="https://www.cs.cmu.edu/~lwehbe/">Leila Wehbe</a>
                        <br>
                        <em>ICLR 2024</em>
                        <br>
                        <a href="https://arxiv.org/abs/2310.04420">arxiv
                            page</a> /
                            <a href="data/BrainSCUBA_bib.html">bibtex</a>
                        <p></p>
                        <p>We propose a way to leverage contrastive image-language models (CLIP) and fine-tuned language models to generate natural language descriptions of voxel-wise selectivity in the higher order visual areas.</p>
                    </td>
                </tr>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/Brain_DiVE_teaser_small.jpg" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2306.03089" id="braindive">
                            <span class="papertitle">Brain Diffusion for Visual Exploration: Cortical Discovery using Large Scale Generative Models</span>
                        </a>
                        <br>
                        <strong>Andrew F. Luo</strong>, <a href="https://www.maggie-henderson.com/">Margaret M. Henderson</a>, <a href="https://www.cs.cmu.edu/~lwehbe/">Leila Wehbe*</a>, <a href="https://sites.google.com/andrew.cmu.edu/tarrlab/people/michael-j-tarr">Michael J. Tarr*</a>
                        <br>
                        * Co-corresponding authors
                        <br>
                         <span style="color:orangered;"> <em>NeurIPS 2023</em> oral</span>, (top 0.7% of all submissions)
                        <br>
                        <a href="https://www.cs.cmu.edu/~afluo/BrainDiVE/">project
                            page</a> /
                        <a href="data/BrainDiVE_bib.html">bibtex</a> /
                        <a href="https://github.com/aluo-x/BrainDiVE">code</a>
                        <p></p>
                        <p>We propose a way to generate images that activate regions of the brain by leveraging natural image priors from Diffusion models. </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/Size_teaser.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://www.biorxiv.org/content/10.1101/2023.03.17.533179" id="brainsize">
                            <span class="papertitle">Neural Selectivity for Real-World Object Size In Natural Images</span>
                        </a>
                        <br><strong>Andrew F. Luo</strong>, <a href="https://www.cs.cmu.edu/~lwehbe/">Leila Wehbe</a>, <a href="https://sites.google.com/andrew.cmu.edu/tarrlab/people/michael-j-tarr">Michael J. Tarr</a>, <a href="https://www.maggie-henderson.com/">Margaret M. Henderson</a>
                        <br>
                        <em>BioRxiv, 2023</em> (in submission)
                        <br>
                        <a href="https://www.biorxiv.org/content/10.1101/2023.03.17.533179">bioRxiv page</a> /
                        <a href="data/BrainSize_bib.html">bibtex</a>
                        <p></p>
                        <p>We examine the selectivity of the brain to real-world size in complex natural images.</p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <video autoplay loop muted width="160" style="border-style: none"><source src="images/NAFs.mpt.mp4"></video>
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2204.00628" id="NAFs">
                            <span class="papertitle">Learning Neural Acoustic Fields</span>
                        </a>
                        <br>
                        <strong>Andrew F. Luo</strong>, <a href="https://yilundu.github.io/">Yilun Du</a>, <a href="https://sites.google.com/andrew.cmu.edu/tarrlab/people/michael-j-tarr">Michael J. Tarr</a>, <a href="https://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a>, <a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba</a>, <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>
                        <br>
                        <em>NeurIPS 2022</em> (Summer intership at IBM)
                        <br>
                        <a href="https://www.andrew.cmu.edu/user/afluo/Neural_Acoustic_Fields/">project
                            page</a> /
                        <a href="data/NAFs_bib.html">bibtex</a> /
                        <a href="https://github.com/aluo-x/Learning_Neural_Acoustic_Fields">code</a>
                        <p></p>
                        <p>We propose a learnable and compact implicit encoding for acoustic impulse responses. We find that our NAFs can achieve state-of-the-art performance at a tiny size footprint.</p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/moca_teaser.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://openreview.net/forum?id=lY0-7bj0Vfz" id="protoattn">
                            <span class="papertitle">Prototype memory and attention mechanisms for few shot image generation</span>
                        </a>
                        <br>
                        <a href="https://crazy-jack.github.io/">Tianqin Li*</a>, <a href="https://zijieli-jlee.github.io/">Zijie Li*</a>, <strong>Andrew F. Luo</strong>, Harold Rockwell, <a href="https://sites.google.com/view/barati">Amir Barati Farimani</a>, <a href="https://www.cnbc.cmu.edu/~tai/">Tai Sing Lee</a>

                        <br>
                        <em>ICLR 2022</em>
                        <br>
                        <a href="data/moca_bib.html">bibtex</a> /
                        <a href="https://github.com/Crazy-Jack/MoCA_release">code</a>
                        <p></p>
                        <p>We show that having a prototype memory with attention mechanisms can improve image synthesis quality, and learn interpretable visual concept clusters.</p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/surfgen_teaser.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2201.00112" id="surfgen">
                            <span class="papertitle">SurfGen: Adversarial 3D Shape Synthesis with Explicit Surface Discriminators</span>
                        </a>
                        <br>
                        <strong>Andrew F. Luo</strong>, <a href="https://crazy-jack.github.io">Tianqin Li</a>, <a href="https://scholar.google.com/citations?user=TqGPd9QAAAAJ&hl=en">Wen-Hao Zhang</a>, <a href="https://www.cnbc.cmu.edu/~tai/">Tai Sing Lee</a>
                        <br>
                        <em>ICCV 2021</em>
                        <br>
                        <a href="https://arxiv.org/abs/2201.00112">arxiv page</a> /
                        <a href="data/surfgen_bib.html">bibtex</a> /
                        <a href="https://github.com/aluo-x/NeuralRaycaster">code</a>
                        <p></p>
                        <p>We propose a surface based discriminator for implicit shape generation. Our discriminator uses differentiable ray-casting and marching cubes.</p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/3dsln.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2007.11744" id="3dsln">
                            <span class="papertitle">End-to-End Optimization of Scene Layout</span>
                        </a>
                        <br>
                        <strong>Andrew F. Luo</strong>, <a href="http://ztzhang.info/">Zhoutong Zhang</a>, <a href="http://jiajunwu.com/">Jiajun Wu</a>, <a href="https://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a>
                        <br>
                        <span style="color:orangered;"> <em>CVPR 2020</em> oral</span>
                        <br>
                        <a href="http://3dsln.csail.mit.edu/">project page</a> /
                        <a href="data/3dsln_bib.html">bibtex</a> /
                        <a href="https://github.com/aluo-x/3D_SLN">code</a>
                        <p></p>
                        <p>We propose contrained scene synthesis using graph neural networks, we show that generated scenes can be refined using differentiable rendering.</p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/shapeprograms_teaser.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/1901.02875" id="shapeprograms">
                            <span class="papertitle">Learning to Infer and Execute 3D Shape Programs</span>
                        </a>
                        <br>
                        <a href="https://people.csail.mit.edu/yonglong/">Yonglong Tian</a>, <strong>Andrew F. Luo</strong>, <a href="https://xingyuansun.com/">Xingyuan Sun</a>, <a href="https://www.cs.cornell.edu/~ellisk/">Kevin Ellis</a>, <a href="https://billf.mit.edu/">William T. Freeman</a>, <a href="https://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a>, <a href="http://jiajunwu.com">Jiajun Wu</a>

                        <br>
                        <em>ICLR 2019</em>
                        <br>
                        <a href="http://shape2prog.csail.mit.edu/">project
                            page</a> /
                        <a href="data/shapeprograms_bib.html">bibtex</a> /
                        <a href="https://github.com/HobbitLong/shape2prog">code</a>
                        <p></p>
                        <p>We propose a learnable decomposition of 3D shapes into symbolic programs that can be executed.</p>
                    </td>
                </tr>
                </tbody></table>
                </tbody>
            </table>
</body>
</html>
