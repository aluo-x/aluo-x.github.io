<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Andrew Luo</title>

    <meta name="author" content="Andrew Luo">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr style="padding:0px">
                    <td style="padding:2.5%;width:63%;vertical-align:middle">
                        <p class="name" style="text-align: center;">
                            Andrew Luo
                        </p>
                        <p> I am an incoming Assistant Professor at the University of Hong Kong at a joint position between the <a href="https://datascience.hku.hk/">Musketeers Foundation Institute of Data Science</a> and the <a href="https://psychology.hku.hk/">Psychology Department</a>.
                        </p>
                        <p> Formerly a PhD student at <a href="https://www.cmu.edu/">Carnegie Mellon University</a> (defended Sept 2024), affiliated with the Machine Learning Department and the Neuroscience Institute.
                            I completed a <a href="https://www.ml.cmu.edu/academics/joint-ml-phd.html">joint PhD in Neural
                            Computation & Machine Learning</a>. I was co-advised by <a
                                href="https://www.cs.cmu.edu/~lwehbe/">Leila Wehbe</a> and <a
                                href="https://www.cmu.edu/dietrich/psychology/directory/core-training-faculty/tarr-michael.html">Michael
                            Tarr</a>. I earned my undergraduate degree in Computer Science from MIT in 2019, and a Master of Science in Machine Learning Research from CMU. </p><p>My current
                            research focuses on computational methods for studying visual perception in the human brain, with experience in 3D generative models.
                        </p>
                        <p style="text-align:center">
                            <a href="mailto:luo_andrew@outlook.com">Email</a> &nbsp;/&nbsp;
                            <a href="mailto:afluo@andrew.cmu.edu">CMU Email</a> &nbsp;/&nbsp;
                            <!--                  <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp;-->
                            <!--                  <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp;-->
                            <a href="https://scholar.google.com/citations?user=bWYvvkUAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                            <!--                  <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp;-->
                            <a href="https://github.com/aluo-x">Github</a>&nbsp;/&nbsp;
                            <a href="./wechat.html">WeChat</a>
                        </p>
                    </td>
                    <td style="padding:2.5%;width:40%;max-width:40%">
                        <img style="width:100%;max-width:100%" alt="profile photo"
                                                            src="images/aluo_circle_small.png" class="hoverZoomLink">
                    </td>
                </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Research</h2>
                        <p>
                            I'm interested in generative models and their applications in studying the brain. My current
                            research focuses on semantic divisions within the human visual cortex.
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="./images/BrainSCUBA_teaser.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2310.04420" id="brainscuba">
                            <span class="papertitle">BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity</span>
                        </a>
                        <br>
                        <strong>Andrew F. Luo</strong>, <a href="https://www.maggie-henderson.com/">Margaret M. Henderson</a>, <a href="https://sites.google.com/andrew.cmu.edu/tarrlab/people/michael-j-tarr">Michael J. Tarr</a>, <a href="https://www.cs.cmu.edu/~lwehbe/">Leila Wehbe</a>
                        <br>
                        <em>ICLR 2024</em>
                        <br>
                        <a href="https://arxiv.org/abs/2310.04420">arxiv
                            page</a> /
                            <a href="data/BrainSCUBA_bib.html">bibtex</a>
                        <p></p>
                        <p>We propose a way to leverage contrastive image-language models (CLIP) and fine-tuned language models to generate natural language descriptions of voxel-wise selectivity in the higher order visual areas.</p>
                    </td>
                </tr>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/Brain_DiVE_teaser.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2306.03089" id="braindive">
                            <span class="papertitle">Brain Diffusion for Visual Exploration: Cortical Discovery using Large Scale Generative Models</span>
                        </a>
                        <br>
                        <strong>Andrew F. Luo</strong>, <a href="https://www.maggie-henderson.com/">Margaret M. Henderson</a>, <a href="https://www.cs.cmu.edu/~lwehbe/">Leila Wehbe*</a>, <a href="https://sites.google.com/andrew.cmu.edu/tarrlab/people/michael-j-tarr">Michael J. Tarr*</a>
                        <br>
                        * Co-corresponding authors
                        <br>
                         <span style="color:orangered;"> <em>NeurIPS 2023</em> oral</span>, (top 0.7% of all submissions)
                        <br>
                        <a href="https://www.cs.cmu.edu/~afluo/BrainDiVE/">project
                            page</a> /
                        <a href="data/BrainDiVE_bib.html">bibtex</a> /
                        <a href="https://github.com/aluo-x/BrainDiVE">code</a>
                        <p></p>
                        <p>We propose a way to generate images that activate regions of the brain by leveraging natural image priors from Diffusion models. </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/Size_teaser.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://www.biorxiv.org/content/10.1101/2023.03.17.533179" id="brainsize">
                            <span class="papertitle">Neural Selectivity for Real-World Object Size In Natural Images</span>
                        </a>
                        <br><strong>Andrew F. Luo</strong>, <a href="https://www.cs.cmu.edu/~lwehbe/">Leila Wehbe</a>, <a href="https://sites.google.com/andrew.cmu.edu/tarrlab/people/michael-j-tarr">Michael J. Tarr</a>, <a href="https://www.maggie-henderson.com/">Margaret M. Henderson</a>
                        <br>
                        <em>BioRxiv, 2023</em> (in submission)
                        <br>
                        <a href="https://www.biorxiv.org/content/10.1101/2023.03.17.533179">bioRxiv page</a> /
                        <a href="data/BrainSize_bib.html">bibtex</a>
                        <p></p>
                        <p>We examine the selectivity of the brain to real-world size in complex natural images.</p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <video autoplay loop muted width="160" style="border-style: none"><source src="images/NAFs.mpt.mp4"></video>
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2204.00628" id="NAFs">
                            <span class="papertitle">Learning Neural Acoustic Fields</span>
                        </a>
                        <br>
                        <strong>Andrew F. Luo</strong>, <a href="https://yilundu.github.io/">Yilun Du</a>, <a href="https://sites.google.com/andrew.cmu.edu/tarrlab/people/michael-j-tarr">Michael J. Tarr</a>, <a href="https://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a>, <a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba</a>, <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>
                        <br>
                        <em>NeurIPS 2022</em> (Summer intership at IBM)
                        <br>
                        <a href="https://www.andrew.cmu.edu/user/afluo/Neural_Acoustic_Fields/">project
                            page</a> /
                        <a href="data/NAFs_bib.html">bibtex</a> /
                        <a href="https://github.com/aluo-x/Learning_Neural_Acoustic_Fields">code</a>
                        <p></p>
                        <p>We propose a learnable and compact implicit encoding for acoustic impulse responses. We find that our NAFs can achieve state-of-the-art performance at a tiny size footprint.</p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/moca_teaser.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://openreview.net/forum?id=lY0-7bj0Vfz" id="protoattn">
                            <span class="papertitle">Prototype memory and attention mechanisms for few shot image generation</span>
                        </a>
                        <br>
                        <a href="https://crazy-jack.github.io/">Tianqin Li*</a>, <a href="https://zijieli-jlee.github.io/">Zijie Li*</a>, <strong>Andrew F. Luo</strong>, Harold Rockwell, <a href="https://sites.google.com/view/barati">Amir Barati Farimani</a>, <a href="https://www.cnbc.cmu.edu/~tai/">Tai Sing Lee</a>

                        <br>
                        <em>ICLR 2022</em>
                        <br>
                        <a href="data/moca_bib.html">bibtex</a> /
                        <a href="https://github.com/Crazy-Jack/MoCA_release">code</a>
                        <p></p>
                        <p>We show that having a prototype memory with attention mechanisms can improve image synthesis quality, and learn interpretable visual concept clusters.</p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/surfgen_teaser.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2201.00112" id="surfgen">
                            <span class="papertitle">SurfGen: Adversarial 3D Shape Synthesis with Explicit Surface Discriminators</span>
                        </a>
                        <br>
                        <strong>Andrew F. Luo</strong>, <a href="https://crazy-jack.github.io">Tianqin Li</a>, <a href="https://scholar.google.com/citations?user=TqGPd9QAAAAJ&hl=en">Wen-Hao Zhang</a>, <a href="https://www.cnbc.cmu.edu/~tai/">Tai Sing Lee</a>
                        <br>
                        <em>ICCV 2021</em>
                        <br>
                        <a href="https://arxiv.org/abs/2201.00112">arxiv page</a> /
                        <a href="data/surfgen_bib.html">bibtex</a> /
                        <a href="https://github.com/aluo-x/NeuralRaycaster">code</a>
                        <p></p>
                        <p>We propose a surface based discriminator for implicit shape generation. Our discriminator uses differentiable ray-casting and marching cubes.</p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/3dsln.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2007.11744" id="3dsln">
                            <span class="papertitle">End-to-End Optimization of Scene Layout</span>
                        </a>
                        <br>
                        <strong>Andrew F. Luo</strong>, <a href="http://ztzhang.info/">Zhoutong Zhang</a>, <a href="http://jiajunwu.com/">Jiajun Wu</a>, <a href="https://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a>
                        <br>
                        <span style="color:orangered;"> <em>CVPR 2020</em> oral</span>
                        <br>
                        <a href="http://3dsln.csail.mit.edu/">project page</a> /
                        <a href="data/3dsln_bib.html">bibtex</a> /
                        <a href="https://github.com/aluo-x/3D_SLN">code</a>
                        <p></p>
                        <p>We propose contrained scene synthesis using graph neural networks, we show that generated scenes can be refined using differentiable rendering.</p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/shapeprograms_teaser.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/1901.02875" id="shapeprograms">
                            <span class="papertitle">Learning to Infer and Execute 3D Shape Programs</span>
                        </a>
                        <br>
                        <a href="https://people.csail.mit.edu/yonglong/">Yonglong Tian</a>, <strong>Andrew F. Luo</strong>, <a href="https://xingyuansun.com/">Xingyuan Sun</a>, <a href="https://www.cs.cornell.edu/~ellisk/">Kevin Ellis</a>, <a href="https://billf.mit.edu/">William T. Freeman</a>, <a href="https://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a>, <a href="http://jiajunwu.com">Jiajun Wu</a>

                        <br>
                        <em>ICLR 2019</em>
                        <br>
                        <a href="http://shape2prog.csail.mit.edu/">project
                            page</a> /
                        <a href="data/shapeprograms_bib.html">bibtex</a> /
                        <a href="https://github.com/HobbitLong/shape2prog">code</a>
                        <p></p>
                        <p>We propose a learnable decomposition of 3D shapes into symbolic programs that can be executed.</p>
                    </td>
                </tr>
                </tbody></table>
                </tbody>
            </table>
</body>
</html>
