<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Andrew Luo</title>

    <meta name="author" content="Andrew Luo">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr class="profile-row" style="padding:0px">
                    <td style="padding:2.5%;width:65%;vertical-align:middle">
                        <p class="name" style="text-align: center;">
                            Andrew Luo
                        </p>
                        <img src="images/profile_2.jpg" class="mobile-photo" alt="profile photo">

                        <p> I am a tenure-track Assistant Professor at the <a href="https://www.hku.hk/">University of Hong Kong</a> jointly appointed by <a href="https://datascience.hku.hk/">Musketeers Foundation Institute of Data Science</a> and the <a href="https://psychology.hku.hk/people/andrew-luo/">Psychology Department</a>.
                        </p>
                        <p>
                            I received my <a href="https://www.ml.cmu.edu/academics/joint-ml-phd.html#:~:text=PhD%20in%20Neural%20Computation%20%26%20Machine%20Learning">joint PhD in <u>Machine Learning</u> & <u>Neural Computation</u></a> from Carnegie Mellon University (CMU) in 2024, where I worked with <a href="https://www.cmu.edu/dietrich/psychology/directory/core-training-faculty/tarr-michael.html">Michael
                            Tarr</a> and <a
                                href="https://www.cs.cmu.edu/~lwehbe/">Leila Wehbe</a>. Before that, I earned my undergraduate degree in Computer Science from the Massachusetts Institute of Technology (MIT) in 2019. I also have a Master of Science in Machine Learning Research from CMU. </p><p>My work focuses on understanding the computational principles underlying <u>visual perception </u>and how these principles can inform the development of <u> improved generative models and intelligent machines</u>. Ultimately, I aim to bridge the gap between human and machine reasoning, leading to both a deeper understanding of human cognition and advancements in artificial intelligence.


                        </p>
<!--                        <p style="font-size: 18px;color: seagreen">-->
<!--                            PhD student recruitment focus: Brain Decoding & <br> Robotic Learning/Planning/Control & 3D Reconstruction for Planning and Scene Understanding & Model Interpretability and Generative Model Theory.-->
<!--                        </p>-->
<!--                        <p style="font-size: 18px;color: seagreen">For 2024 Winter and 2025 Fall &#45;&#45; I am recruiting PhDs candidates with a background in <u>computer vision</u>, <u>AI for Neuroscience (NeuroAI)</u>, and <u>image generative models</u> to join my research group (HKU PhD Admission). I also welcome RAs (remote or in-person) or remote collaboration with PhDs, master's, and undergraduates. Please send an email to aluo@hku.hk with a copy of your CV and a short statement about your interests.</p>-->
<!--                        <p style="font-size: 18px;color: seagreen">2024 Nov 25 update: I have completed PhD student hiring for IDS this upcoming cycle. I continue to recruit PhD applicants through the Psychology department.</p>-->
                        <p style="font-size: 18px;color: mediumvioletred">I am recruiting PhDs for 2026 Fall now. (Updated Dec 1, 2025)</p> <p style="font-size: 18px;color: seagreen">I welcome RAs (remote or in-person) or remote collaboration with PhDs, master's, and undergraduates.</p>

                        <p style="text-align:center">
                            <a href="mailto:luo_andrew@outlook.com">Email</a> &nbsp;/&nbsp;
                            <a href="mailto:aluo@hku.hk">HKU Email</a> &nbsp;/&nbsp;
                            <a href="https://scholar.google.com/citations?hl=en&user=bWYvvkUAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp;/&nbsp;
                            <!--                  <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp;-->
                            <a href="https://github.com/aluo-x">Github</a>&nbsp;/&nbsp;
                            <a href="./wechat.html">WeChat</a> /
                            <a href="./chinese.html">中文</a>
                        </p>
                    </td>
                    <td class="photo-cell" style="padding:2.5%;width:40%;max-width:40%">
        <img style="width:100%;max-width:100%" alt="profile photo" src="images/profile_2.jpg" class="hoverZoomLink">
    </td>
                </tr>
                </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
                <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Students (sorted by year and last name)</h2>
                <ul style="list-style-type: none; padding-left: 0; margin-top: 10px;">

                    <!-- Student 1 -->
                    <li style="margin-bottom: 10px;">
                        <strong><a href="https://sagecao1125.github.io/"> Jiahang Cao</a> (<a href="mailto:jiahangcao@connect.hku.hk">email</a>)</strong> &nbsp;/&nbsp; PhD (starting 2025). HKUST (GZ) MSc
                    </li>

                    <!-- Student 2 -->
                    <li style="margin-bottom: 10px;">
                        <strong><a href="https://michaelmaiii.github.io/"> Weijian Mai</a> (<a href="mailto:weijian.mai@connect.hku.hk">email</a>)</strong> &nbsp;/&nbsp; PhD (starting 2025). SCUT MSc
                    </li>

                    <!-- Student 3 -->
                    <li style="margin-bottom: 10px;">
                        <strong><a href="https://ezacngm.github.io"> Mu Nan</a> (<a href="mailto:ezacngmpg@connect.hku.hk">email</a>)</strong> &nbsp;/&nbsp; PhD (starting 2025). University of Manchester MSc
                    </li>

                    <!-- Student 4 -->
                    <li style="margin-bottom: 10px;">
                        <strong><a href="https://kiddoray.github.io/">Rui Zhang</a> (<a href="mailto:ruizhang97@connect.hku.hk">email</a>)</strong> &nbsp;/&nbsp; PhD (starting 2025). Fudan MSc
                    </li>

                </ul>
            </td>
        </tr>
    </tbody>
</table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
                <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Collaborators (sorted by year and last name)</h2>
                <ul style="list-style-type: none; padding-left: 0; margin-top: 10px;">

                    <!-- Student 1 -->
                    <li style="margin-bottom: 10px;">
                        <strong>Muquan Yu (<a href="mailto:leomqyu@outlook.com">email</a>)</strong> &nbsp;/&nbsp; RA 2025; Now HKU PhD
                    </li>

                    <!-- Student 2 -->
                    <li style="margin-bottom: 10px;">
                        <strong>Yinjie Chen (<a href="mailto:maxwellcaffrey915@gmail.com">email</a>)</strong> &nbsp;/&nbsp; RA 2025; Now applying to PhD
                    </li>

                    <!-- Student 3 -->
                    <li style="margin-bottom: 10px;">
                        <strong>Zipeng Yan (<a href="mailto:kelvinyzp@gmail.com">email</a>)</strong> &nbsp;/&nbsp; RA 2025; Now in industry
                    </li>

                    <!-- Student 4 -->

                </ul>
            </td>
        </tr>
    </tbody>
</table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Research</h2>
                        <p>
                            I am interested in understanding human perception and building better generative models and machines that are capable of human-like reasoning.
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="./images/compose_teaser.jpg" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2510.01068" id="compose">
                            <span class="papertitle">Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition
</span>
                        </a>
                        <br>
                        Jiahang Cao*, Yize Huang*, Hanzhong Guo, Rui Zhang, Mu Nan, Weijian Mai, Jiaxu Wang, Hao Cheng, Jingkai Sun, Gang Han, Wen Zhao, Qiang Zhang, Yijie Guo, Qihao Zheng, Chunfeng Song, Xiao Li, Ping Luo, <strong>Andrew F. Luo</strong>
                        <br>
                        * Co-first authors
                        <br>

                        <span><em>In submission</em></span>
                        <br>
                        <a href="https://arxiv.org/abs/2510.01068">arxiv
                            page</a> /
                            <a href="data/compose_bib.html">bibtex</a>
                        <p></p>
                        <p>We show that composition of diffusion policies can yield consistent improvements across a diverse set of tasks. By utilizing test-time search, we can obtain a policy that is stronger than parent policies by seeking out consensus regions.</p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="./images/ph_reg_teaser.jpg" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2505.21501" id="phreg">
                            <span class="papertitle">Vision Transformers with Self-Distilled Registers</span>
                        </a>
                        <br>
                        <a href="https://openreview.net/profile?id=~Yinjie_Chen3">Yinjie Chen*</a>, <a href="https://openreview.net/profile?id=~Zipeng_Yan1">Zipeng Yan*</a>, <a href="http://chongzhou96.github.io/">Chong Zhou</a>, <a href="http://daibo.info/">Bo Dai</a>, <strong>Andrew F. Luo</strong>
                        <br>
                        * Co-first authors
                        <br>

                        <span style="color:orangered;"> <em>NeurIPS 2025</em> Spotlight</span>
                        <br>
                        <a href="https://arxiv.org/abs/2505.21501">arxiv
                            page</a> /
                            <a href="data/PH_reg_bib.html">bibtex</a>
                        <p></p>
                        <p>We show that artifacts can be removed from pre-trained ViTs without any labeled data by introducing registers in post-training. Our method uses a model combined with test-time augmentation to distill itself, leading to significant improvements in open-vocabulary segmentation and dense prediction tasks.</p>
                    </td>
                </tr>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="./images/Brain_CoRL_teaser.jpg" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2505.15813" id="braincorl">
                            <span class="papertitle">Meta-Learning an In-Context Transformer Model of Human Higher Visual Cortex
</span>
                        </a>
                        <br>
                        <a href="https://scholar.google.com/citations?hl=en&user=8fm4uE8AAAAJ">Muquan Yu</a>, <a href="https://openreview.net/profile?id=~Mu_Nan2">Mu Nan</a>, <a href="https://hosseinadeli.github.io/">Hossein Adeli</a>, <a href="https://jacob-prince.github.io/">Jacob S. Prince</a>, <a href="https://psych.uw.edu/people/9200">John A. Pyles</a>, <a href="https://www.cs.cmu.edu/~lwehbe/">Leila Wehbe</a>, <a href="https://www.maggie-henderson.com/">Margaret M. Henderson</a>, <a href="https://sites.google.com/andrew.cmu.edu/tarrlab/people/michael-j-tarr">Michael J. Tarr</a>, <strong>Andrew F. Luo</strong>
                        <br>

                        <em>NeurIPS 2025</em>
                        <br>
                        <a href="https://arxiv.org/abs/2505.15813">arxiv
                            page</a> /
                            <a href="data/Braincorl_bib.html">bibtex</a>
                        <p></p>
                        <p>We show how to construct higher visual cortex encoders that can generalize across subjects, scanners, voxel sizes, protocols, and images without any additional finetuning by using meta-learning across subjects and in-context learning across stimuli.</p>
                    </td>
                </tr>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="./images/insilico.jpg" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2510.21142" id="brain_silico">
                            <span class="papertitle">In Silico Mapping of Visual Categorical Selectivity Across the Whole Brain</span>
                        </a>
                        <br>
                        <a href="https://x.com/ethanhwang_">Ethan Hwang</a>, <a href="https://hosseinadeli.github.io/">Hossein Adeli</a>, <a href="https://psychology.columbia.edu/content/wenxuan-guo">Wenxuan Guo</a>, <strong>Andrew F. Luo</strong>, <a href="https://zuckermaninstitute.columbia.edu/nikolaus-kriegeskorte-phd">Nikolaus Kriegeskorte</a>
                        <br>

                        <span><em>NeurIPS 2025</em></span>
                        <br>
                        <a href="https://arxiv.org/abs/2510.21142">arxiv
                            page</a> /
                            <a href="data/brainsilico_bib.html">bibtex</a>
                        <p></p>
                        <p>Computational tests of selectivity using more accurate transformer encoders of the brain.</p>
                    </td>
                </tr>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="./images/brainnerds_compressed.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2406.02659" id="brainnerd">
                            <span class="papertitle">Reanimating Images using Neural Representations of Dynamic Stimuli</span>
                        </a>
                        <br>
                        <a href="https://jacobyeung.org/">Jacob Yeung</a>, <strong>Andrew F. Luo</strong>, <a href="https://gabesarch.me/">Gabriel Sarch</a>, <a href="https://www.maggie-henderson.com/">Margaret M. Henderson</a>, <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a>, <a href="https://sites.google.com/andrew.cmu.edu/tarrlab/people/michael-j-tarr">Michael J. Tarr</a>
                        <br>

                        <span style="color:orangered;"> <em>CVPR 2025</em> Oral</span>
                        <br>
                        <a href="https://arxiv.org/abs/2406.02659">arxiv
                            page</a> /
                            <a href="data/BrainNerd_bib.html">bibtex</a>
                        <p></p>
                        <p>We propose image-conditioned decoding of perceived motion from fMRI data, we show that this can be used to animate images with an video diffusion model.</p>
                    </td>
                </tr>


                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="./images/brainsail_teaser.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2410.05266" id="brainsail">
                            <span class="papertitle">Brain Mapping with Dense Features: Grounding Cortical Semantic Selectivity in Natural Images With Vision Transformers</span>
                        </a>
                        <br>
                        <strong>Andrew F. Luo</strong>, <a href="https://jacobyeung.org/">Jacob Yeung</a>, <a href="https://github.com/RBZ-99">Rushikesh Zawar</a>, <a href="https://srdewan.github.io/">Shaurya Dewan</a>, <a href="https://www.maggie-henderson.com/">Margaret M. Henderson</a>, <a href="https://www.cs.cmu.edu/~lwehbe/">Leila Wehbe*</a>, <a href="https://sites.google.com/andrew.cmu.edu/tarrlab/people/michael-j-tarr">Michael J. Tarr*</a>
                        <br>
                        * Co-corresponding authors
                        <br>
                        <em>ICLR 2025</em>
                        <br>
                        <a href="https://arxiv.org/abs/2410.05266">arxiv
                            page</a> /
                            <a href="data/BrainSAIL_bib.html">bibtex</a>
                        <p></p>
                        <p>We propose an efficient gradient-free distillation module capable of extraction high quality dense CLIP embeddings, and utilize these embeddings to understand semantic selectivity in the visual cortex.</p>
                    </td>
                </tr>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="./images/audionav.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2407.11333" id="audionav">
                            <span class="papertitle">Disentangled Acoustic Fields For Multimodal Physical Scene Understanding</span>
                        </a>
                        <br>
                        <a href="https://github.com/sjtuyinjie">Jie Yin</a>, <strong>Andrew F. Luo</strong>, <a href="https://yilundu.github.io/">Yilun Du</a>, <a href="https://www.merl.com/people/cherian"> Anoop Cherian</a>, <a href="https://www.merl.com/people/tmarks">Tim K Marks</a>, <a href="https://www.jonathanleroux.org/">Jonathan Le Roux</a>, <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>
                        <br>
                        <em>IROS 2024</em>
                        <br>
                        <a href="https://arxiv.org/abs/2407.11333">arxiv
                            page</a> /
                            <a href="data/AudioNav_bib.html">bibtex</a>
                        <p></p>
                        <p>We investigate the problem of visual-acoustic navigation conditioned on a continuous acoustic field representation of audio.</p>
                    </td>
                </tr>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="./images/diffusionpid.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2406.05191" id="diffusionpid">
                            <span class="papertitle">DiffusionPID: Interpreting Diffusion via Partial Information Decomposition</span>
                        </a>
                        <br>
                        <a href="https://srdewan.github.io/">Shaurya Dewan</a>, <a href="https://github.com/RBZ-99">Rushikesh Zawar</a>, <a href="https://scholar.google.com/citations?user=3Qqm9ycAAAAJ&hl=en">Prakanshul Saxena</a>, <a href="https://zdxdsw.github.io/">Yingshan Chang</a>, <strong>Andrew F. Luo</strong>, <a href="https://talkingtorobots.com/yonatanbisk.html">Yonatan Bisk</a>
                        <br>
                        <em>NeurIPS 2024</em>
                        <br>
                        <a href="https://arxiv.org/abs/2406.05191">arxiv
                            page</a> /
                            <a href="data/diffusionpid_bib.html">bibtex</a>
                        <p></p>
                        <p>We leverage ideas from information theory to understand the contributions of individual text tokens and their interactions when generating images.</p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="./images/BrainSCUBA_teaser.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2310.04420" id="brainscuba">
                            <span class="papertitle">BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity</span>
                        </a>
                        <br>
                        <strong>Andrew F. Luo</strong>, <a href="https://www.maggie-henderson.com/">Margaret M. Henderson</a>, <a href="https://sites.google.com/andrew.cmu.edu/tarrlab/people/michael-j-tarr">Michael J. Tarr</a>, <a href="https://www.cs.cmu.edu/~lwehbe/">Leila Wehbe</a>
                        <br>
                        <em>ICLR 2024</em>
                        <br>
                        <a href="https://arxiv.org/abs/2310.04420">arxiv
                            page</a> /
                            <a href="data/BrainSCUBA_bib.html">bibtex</a>
                        <p></p>
                        <p>We propose a way to leverage contrastive image-language models (CLIP) and fine-tuned language models to generate natural language descriptions of voxel-wise selectivity in the higher order visual areas.</p>
                    </td>
                </tr>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/Brain_DiVE_teaser_small.jpg" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2306.03089" id="braindive">
                            <span class="papertitle">Brain Diffusion for Visual Exploration: Cortical Discovery using Large Scale Generative Models</span>
                        </a>
                        <br>
                        <strong>Andrew F. Luo</strong>, <a href="https://www.maggie-henderson.com/">Margaret M. Henderson</a>, <a href="https://www.cs.cmu.edu/~lwehbe/">Leila Wehbe*</a>, <a href="https://sites.google.com/andrew.cmu.edu/tarrlab/people/michael-j-tarr">Michael J. Tarr*</a>
                        <br>
                        * Co-corresponding authors
                        <br>
                         <span style="color:orangered;"> <em>NeurIPS 2023</em> Oral</span>, (top 0.7% of all submissions)
                        <br>
                        <a href="https://www.cs.cmu.edu/~afluo/BrainDiVE/">project
                            page</a> /
                        <a href="data/BrainDiVE_bib.html">bibtex</a> /
                        <a href="https://github.com/aluo-x/BrainDiVE">code</a>
                        <p></p>
                        <p>We propose a way to generate images that activate regions of the brain by leveraging natural image priors from Diffusion models. </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/Size_teaser.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://www.biorxiv.org/content/10.1101/2023.03.17.533179" id="brainsize">
                            <span class="papertitle">Neural Selectivity for Real-World Object Size In Natural Images</span>
                        </a>
                        <br><strong>Andrew F. Luo</strong>, <a href="https://www.cs.cmu.edu/~lwehbe/">Leila Wehbe</a>, <a href="https://sites.google.com/andrew.cmu.edu/tarrlab/people/michael-j-tarr">Michael J. Tarr</a>, <a href="https://www.maggie-henderson.com/">Margaret M. Henderson</a>
                        <br>
                        <em>BioRxiv, 2023</em> (in submission)
                        <br>
                        <a href="https://www.biorxiv.org/content/10.1101/2023.03.17.533179">bioRxiv page</a> /
                        <a href="data/BrainSize_bib.html">bibtex</a>
                        <p></p>
                        <p>We examine the selectivity of the brain to real-world size in complex natural images.</p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <video autoplay loop muted width="160" style="border-style: none"><source src="images/NAFs.mpt.mp4"></video>
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2204.00628" id="NAFs">
                            <span class="papertitle">Learning Neural Acoustic Fields</span>
                        </a>
                        <br>
                        <strong>Andrew F. Luo</strong>, <a href="https://yilundu.github.io/">Yilun Du</a>, <a href="https://sites.google.com/andrew.cmu.edu/tarrlab/people/michael-j-tarr">Michael J. Tarr</a>, <a href="https://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a>, <a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba</a>, <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>
                        <br>
                        <em>NeurIPS 2022</em> (Summer intership at IBM)
                        <br>
                        <a href="https://www.andrew.cmu.edu/user/afluo/Neural_Acoustic_Fields/">project
                            page</a> /
                        <a href="data/NAFs_bib.html">bibtex</a> /
                        <a href="https://github.com/aluo-x/Learning_Neural_Acoustic_Fields">code</a>
                        <p></p>
                        <p>We propose a learnable and compact implicit encoding for acoustic impulse responses. We find that our NAFs can achieve state-of-the-art performance at a tiny size footprint.</p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/moca_teaser.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://openreview.net/forum?id=lY0-7bj0Vfz" id="protoattn">
                            <span class="papertitle">Prototype memory and attention mechanisms for few shot image generation</span>
                        </a>
                        <br>
                        <a href="https://crazy-jack.github.io/">Tianqin Li*</a>, <a href="https://zijieli-jlee.github.io/">Zijie Li*</a>, <strong>Andrew F. Luo</strong>, <a href="https://cns.uchicago.edu/program/students/harold-rockwell">Harold Rockwell</a>, <a href="https://sites.google.com/view/barati">Amir Barati Farimani</a>, <a href="https://www.cnbc.cmu.edu/~tai/">Tai Sing Lee</a>

                        <br>
                        <em>ICLR 2022</em>
                        <br>
                        <a href="data/moca_bib.html">bibtex</a>
                        <a href="https://github.com/Crazy-Jack/MoCA_release">code</a>
                        <p></p>
                        <p>We show that having a prototype memory with attention mechanisms can improve image synthesis quality, and learn interpretable visual concept clusters.</p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/surfgen_teaser.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2201.00112" id="surfgen">
                            <span class="papertitle">SurfGen: Adversarial 3D Shape Synthesis with Explicit Surface Discriminators</span>
                        </a>
                        <br>
                        <strong>Andrew F. Luo</strong>, <a href="https://crazy-jack.github.io">Tianqin Li</a>, <a href="https://scholar.google.com/citations?user=TqGPd9QAAAAJ&hl=en">Wen-Hao Zhang</a>, <a href="https://www.cnbc.cmu.edu/~tai/">Tai Sing Lee</a>
                        <br>
                        <em>ICCV 2021</em>
                        <br>
                        <a href="https://arxiv.org/abs/2201.00112">arxiv page</a> /
                        <a href="data/surfgen_bib.html">bibtex</a> /
                        <a href="https://github.com/aluo-x/NeuralRaycaster">code</a>
                        <p></p>
                        <p>We propose a surface based discriminator for implicit shape generation. Our discriminator uses differentiable ray-casting and marching cubes.</p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/3dsln.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2007.11744" id="3dsln">
                            <span class="papertitle">End-to-End Optimization of Scene Layout</span>
                        </a>
                        <br>
                        <strong>Andrew F. Luo</strong>, <a href="http://ztzhang.info/">Zhoutong Zhang</a>, <a href="http://jiajunwu.com/">Jiajun Wu</a>, <a href="https://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a>
                        <br>
                        <span style="color:orangered;"> <em>CVPR 2020</em> oral</span>
                        <br>
                        <a href="http://3dsln.csail.mit.edu/">project page</a> /
                        <a href="data/3dsln_bib.html">bibtex</a> /
                        <a href="https://github.com/aluo-x/3D_SLN">code</a>
                        <p></p>
                        <p>We propose contrained scene synthesis using graph neural networks, we show that generated scenes can be refined using differentiable rendering.</p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/shapeprograms_teaser.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/1901.02875" id="shapeprograms">
                            <span class="papertitle">Learning to Infer and Execute 3D Shape Programs</span>
                        </a>
                        <br>
                        <a href="https://people.csail.mit.edu/yonglong/">Yonglong Tian</a>, <strong>Andrew F. Luo</strong>, <a href="https://xingyuansun.com/">Xingyuan Sun</a>, <a href="https://www.cs.cornell.edu/~ellisk/">Kevin Ellis</a>, <a href="https://billf.mit.edu/">William T. Freeman</a>, <a href="https://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a>, <a href="http://jiajunwu.com">Jiajun Wu</a>

                        <br>
                        <em>ICLR 2019</em>
                        <br>
                        <a href="http://shape2prog.csail.mit.edu/">project
                            page</a> /
                        <a href="data/shapeprograms_bib.html">bibtex</a> /
                        <a href="https://github.com/HobbitLong/shape2prog">code</a>
                        <p></p>
                        <p>We propose a learnable decomposition of 3D shapes into symbolic programs that can be executed.</p>
                    </td>
                </tr>
                </tbody></table>
                </tbody>
            </table>
</body>
</html>
