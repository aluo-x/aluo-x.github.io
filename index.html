<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Andrew Luo</title>

    <meta name="author" content="Andrew Luo">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr style="padding:0px">
                    <td style="padding:2.5%;width:65%;vertical-align:middle">
                        <p class="name" style="text-align: center;">
                            Andrew Luo
                        </p>
                        <p> I am a tenure-track Assistant Professor at the <a href="https://www.hku.hk/">University of Hong Kong</a> jointly appointed by <a href="https://datascience.hku.hk/">Musketeers Foundation Institute of Data Science</a> and the <a href="https://psychology.hku.hk/people/andrew-luo/">Psychology Department</a> starting in 2024 October.
                        </p>
                        <p>
                            I received my <a href="https://www.ml.cmu.edu/academics/joint-ml-phd.html#:~:text=PhD%20in%20Neural%20Computation%20%26%20Machine%20Learning">joint PhD in <u>Machine Learning</u> & <u>Neural Computation</u></a> from Carnegie Mellon University (CMU) in 2024, where I worked with <a href="https://www.cmu.edu/dietrich/psychology/directory/core-training-faculty/tarr-michael.html">Michael
                            Tarr</a> and <a
                                href="https://www.cs.cmu.edu/~lwehbe/">Leila Wehbe</a>. Before that, I earned my undergraduate degree in Computer Science from the Massachusetts Institute of Technology (MIT) in 2019. I also have a Master of Science in Machine Learning Research from CMU. </p><p>My work focuses on understanding the computational principles underlying <u>visual perception </u>and how these principles can inform the development of <u> improved generative models and intelligent machines</u>. Ultimately, I aim to bridge the gap between human and machine reasoning, leading to both a deeper understanding of human cognition and advancements in artificial intelligence.


                        </p>
<!--                        <p style="font-size: 18px;color: seagreen">For 2024 Winter and 2025 Fall &#45;&#45; I am recruiting PhDs candidates with a background in <u>computer vision</u>, <u>AI for Neuroscience (NeuroAI)</u>, and <u>image generative models</u> to join my research group (HKU PhD Admission). I also welcome RAs (remote or in-person) or remote collaboration with PhDs, master's, and undergraduates. Please send an email to aluo@hku.hk with a copy of your CV and a short statement about your interests.</p>-->
<!--                        <p style="font-size: 18px;color: seagreen">2024 Nov 25 update: I have completed PhD student hiring for IDS this upcoming cycle. I continue to recruit PhD applicants through the Psychology department.</p>-->
                        <p style="font-size: 18px;color: seagreen">I have completed PhD student hiring for the 2025 Fall cycle. I welcome RAs (remote or in-person) or remote collaboration with PhDs, master's, and undergraduates.</p>

                        <p style="text-align:center">
                            <a href="mailto:luo_andrew@outlook.com">Email</a> &nbsp;/&nbsp;
                            <a href="mailto:aluo@hku.hk">HKU Email</a> &nbsp;/&nbsp;
                            <a href="https://scholar.google.com/citations?hl=en&user=bWYvvkUAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp;/&nbsp;
                            <!--                  <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp;-->
                            <a href="https://github.com/aluo-x">Github</a>&nbsp;/&nbsp;
                            <a href="./wechat.html">WeChat</a> /
                            <a href="./chinese.html">中文</a>
                        </p>
                    </td>
                    <td style="padding:2.5%;width:40%;max-width:40%">
                        <img style="width:100%;max-width:100%" alt="profile photo"
                                                            src="images/profile_2.jpg" class="hoverZoomLink">
                    </td>
                </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Research</h2>
                        <p>
                            I am interested in understanding human perception and building better generative models and machines that are capable of human-like reasoning.
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>


                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="./images/brainsail_teaser.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2410.05266" id="brainsail">
                            <span class="papertitle">Brain Mapping with Dense Features: Grounding Cortical Semantic Selectivity in Natural Images With Vision Transformers</span>
                        </a>
                        <br>
                        <strong>Andrew F. Luo</strong>, Jacob Yeung, Rushikesh Zawar, Shaurya Dewan, <a href="https://www.maggie-henderson.com/">Margaret M. Henderson</a>, <a href="https://www.cs.cmu.edu/~lwehbe/">Leila Wehbe*</a>, <a href="https://sites.google.com/andrew.cmu.edu/tarrlab/people/michael-j-tarr">Michael J. Tarr*</a>
                        <br>
                        * Co-corresponding authors
                        <br>
                        <em>ICLR 2025</em>
                        <br>
                        <a href="https://arxiv.org/abs/2410.05266">arxiv
                            page</a> /
                            <a href="data/BrainSAIL_bib.html">bibtex</a>
                        <p></p>
                        <p>We propose an efficient gradient-free distillation module capable of extraction high quality dense CLIP embeddings, and utilize these embeddings to understand semantic selectivity in the visual cortex.</p>
                    </td>
                </tr>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="./images/audionav.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2407.11333" id="audionav">
                            <span class="papertitle">Disentangled Acoustic Fields For Multimodal Physical Scene Understanding</span>
                        </a>
                        <br>
                        Jie Yin, <strong>Andrew F. Luo</strong>, <a href="https://yilundu.github.io/">Yilun Du</a>, <a href="https://www.merl.com/people/cherian"> Anoop Cherian</a>, <a href="https://www.merl.com/people/tmarks">Tim K Marks</a>, <a href="https://www.jonathanleroux.org/">Jonathan Le Roux</a>, <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>
                        <br>
                        <em>IROS 2024</em>
                        <br>
                        <a href="https://arxiv.org/abs/2407.11333">arxiv
                            page</a> /
                            <a href="data/AudioNav_bib.html">bibtex</a>
                        <p></p>
                        <p>We investigate the problem of visual-acoustic navigation conditioned on a continuous acoustic field representation of audio.</p>
                    </td>
                </tr>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="./images/diffusionpid.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2406.05191" id="diffusionpid">
                            <span class="papertitle">DiffusionPID: Interpreting Diffusion via Partial Information Decomposition</span>
                        </a>
                        <br>
                        Shaurya Dewan, Rushikesh Zawar, Prakanshul Saxena, Yingshan Chang, <strong>Andrew F. Luo</strong>, <a href="https://talkingtorobots.com/yonatanbisk.html">Yonatan Bisk</a>
                        <br>
                        <em>NeurIPS 2024</em>
                        <br>
                        <a href="https://arxiv.org/abs/2406.05191">arxiv
                            page</a> /
                            <a href="data/diffusionpid_bib.html">bibtex</a>
                        <p></p>
                        <p>We leverage ideas from information theory to understand the contributions of individual text tokens and their interactions when generating images.</p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="./images/BrainSCUBA_teaser.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2310.04420" id="brainscuba">
                            <span class="papertitle">BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity</span>
                        </a>
                        <br>
                        <strong>Andrew F. Luo</strong>, <a href="https://www.maggie-henderson.com/">Margaret M. Henderson</a>, <a href="https://sites.google.com/andrew.cmu.edu/tarrlab/people/michael-j-tarr">Michael J. Tarr</a>, <a href="https://www.cs.cmu.edu/~lwehbe/">Leila Wehbe</a>
                        <br>
                        <em>ICLR 2024</em>
                        <br>
                        <a href="https://arxiv.org/abs/2310.04420">arxiv
                            page</a> /
                            <a href="data/BrainSCUBA_bib.html">bibtex</a>
                        <p></p>
                        <p>We propose a way to leverage contrastive image-language models (CLIP) and fine-tuned language models to generate natural language descriptions of voxel-wise selectivity in the higher order visual areas.</p>
                    </td>
                </tr>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/Brain_DiVE_teaser_small.jpg" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2306.03089" id="braindive">
                            <span class="papertitle">Brain Diffusion for Visual Exploration: Cortical Discovery using Large Scale Generative Models</span>
                        </a>
                        <br>
                        <strong>Andrew F. Luo</strong>, <a href="https://www.maggie-henderson.com/">Margaret M. Henderson</a>, <a href="https://www.cs.cmu.edu/~lwehbe/">Leila Wehbe*</a>, <a href="https://sites.google.com/andrew.cmu.edu/tarrlab/people/michael-j-tarr">Michael J. Tarr*</a>
                        <br>
                        * Co-corresponding authors
                        <br>
                         <span style="color:orangered;"> <em>NeurIPS 2023</em> oral</span>, (top 0.7% of all submissions)
                        <br>
                        <a href="https://www.cs.cmu.edu/~afluo/BrainDiVE/">project
                            page</a> /
                        <a href="data/BrainDiVE_bib.html">bibtex</a> /
                        <a href="https://github.com/aluo-x/BrainDiVE">code</a>
                        <p></p>
                        <p>We propose a way to generate images that activate regions of the brain by leveraging natural image priors from Diffusion models. </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/Size_teaser.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://www.biorxiv.org/content/10.1101/2023.03.17.533179" id="brainsize">
                            <span class="papertitle">Neural Selectivity for Real-World Object Size In Natural Images</span>
                        </a>
                        <br><strong>Andrew F. Luo</strong>, <a href="https://www.cs.cmu.edu/~lwehbe/">Leila Wehbe</a>, <a href="https://sites.google.com/andrew.cmu.edu/tarrlab/people/michael-j-tarr">Michael J. Tarr</a>, <a href="https://www.maggie-henderson.com/">Margaret M. Henderson</a>
                        <br>
                        <em>BioRxiv, 2023</em> (in submission)
                        <br>
                        <a href="https://www.biorxiv.org/content/10.1101/2023.03.17.533179">bioRxiv page</a> /
                        <a href="data/BrainSize_bib.html">bibtex</a>
                        <p></p>
                        <p>We examine the selectivity of the brain to real-world size in complex natural images.</p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <video autoplay loop muted width="160" style="border-style: none"><source src="images/NAFs.mpt.mp4"></video>
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2204.00628" id="NAFs">
                            <span class="papertitle">Learning Neural Acoustic Fields</span>
                        </a>
                        <br>
                        <strong>Andrew F. Luo</strong>, <a href="https://yilundu.github.io/">Yilun Du</a>, <a href="https://sites.google.com/andrew.cmu.edu/tarrlab/people/michael-j-tarr">Michael J. Tarr</a>, <a href="https://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a>, <a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba</a>, <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>
                        <br>
                        <em>NeurIPS 2022</em> (Summer intership at IBM)
                        <br>
                        <a href="https://www.andrew.cmu.edu/user/afluo/Neural_Acoustic_Fields/">project
                            page</a> /
                        <a href="data/NAFs_bib.html">bibtex</a> /
                        <a href="https://github.com/aluo-x/Learning_Neural_Acoustic_Fields">code</a>
                        <p></p>
                        <p>We propose a learnable and compact implicit encoding for acoustic impulse responses. We find that our NAFs can achieve state-of-the-art performance at a tiny size footprint.</p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/moca_teaser.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://openreview.net/forum?id=lY0-7bj0Vfz" id="protoattn">
                            <span class="papertitle">Prototype memory and attention mechanisms for few shot image generation</span>
                        </a>
                        <br>
                        <a href="https://crazy-jack.github.io/">Tianqin Li*</a>, <a href="https://zijieli-jlee.github.io/">Zijie Li*</a>, <strong>Andrew F. Luo</strong>, Harold Rockwell, <a href="https://sites.google.com/view/barati">Amir Barati Farimani</a>, <a href="https://www.cnbc.cmu.edu/~tai/">Tai Sing Lee</a>

                        <br>
                        <em>ICLR 2022</em>
                        <br>
                        <a href="data/moca_bib.html">bibtex</a> /
                        <a href="https://github.com/Crazy-Jack/MoCA_release">code</a>
                        <p></p>
                        <p>We show that having a prototype memory with attention mechanisms can improve image synthesis quality, and learn interpretable visual concept clusters.</p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/surfgen_teaser.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2201.00112" id="surfgen">
                            <span class="papertitle">SurfGen: Adversarial 3D Shape Synthesis with Explicit Surface Discriminators</span>
                        </a>
                        <br>
                        <strong>Andrew F. Luo</strong>, <a href="https://crazy-jack.github.io">Tianqin Li</a>, <a href="https://scholar.google.com/citations?user=TqGPd9QAAAAJ&hl=en">Wen-Hao Zhang</a>, <a href="https://www.cnbc.cmu.edu/~tai/">Tai Sing Lee</a>
                        <br>
                        <em>ICCV 2021</em>
                        <br>
                        <a href="https://arxiv.org/abs/2201.00112">arxiv page</a> /
                        <a href="data/surfgen_bib.html">bibtex</a> /
                        <a href="https://github.com/aluo-x/NeuralRaycaster">code</a>
                        <p></p>
                        <p>We propose a surface based discriminator for implicit shape generation. Our discriminator uses differentiable ray-casting and marching cubes.</p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/3dsln.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/2007.11744" id="3dsln">
                            <span class="papertitle">End-to-End Optimization of Scene Layout</span>
                        </a>
                        <br>
                        <strong>Andrew F. Luo</strong>, <a href="http://ztzhang.info/">Zhoutong Zhang</a>, <a href="http://jiajunwu.com/">Jiajun Wu</a>, <a href="https://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a>
                        <br>
                        <span style="color:orangered;"> <em>CVPR 2020</em> oral</span>
                        <br>
                        <a href="http://3dsln.csail.mit.edu/">project page</a> /
                        <a href="data/3dsln_bib.html">bibtex</a> /
                        <a href="https://github.com/aluo-x/3D_SLN">code</a>
                        <p></p>
                        <p>We propose contrained scene synthesis using graph neural networks, we show that generated scenes can be refined using differentiable rendering.</p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/shapeprograms_teaser.png" alt="PontTuset" width="160" style="border-style: none">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://arxiv.org/abs/1901.02875" id="shapeprograms">
                            <span class="papertitle">Learning to Infer and Execute 3D Shape Programs</span>
                        </a>
                        <br>
                        <a href="https://people.csail.mit.edu/yonglong/">Yonglong Tian</a>, <strong>Andrew F. Luo</strong>, <a href="https://xingyuansun.com/">Xingyuan Sun</a>, <a href="https://www.cs.cornell.edu/~ellisk/">Kevin Ellis</a>, <a href="https://billf.mit.edu/">William T. Freeman</a>, <a href="https://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a>, <a href="http://jiajunwu.com">Jiajun Wu</a>

                        <br>
                        <em>ICLR 2019</em>
                        <br>
                        <a href="http://shape2prog.csail.mit.edu/">project
                            page</a> /
                        <a href="data/shapeprograms_bib.html">bibtex</a> /
                        <a href="https://github.com/HobbitLong/shape2prog">code</a>
                        <p></p>
                        <p>We propose a learnable decomposition of 3D shapes into symbolic programs that can be executed.</p>
                    </td>
                </tr>
                </tbody></table>
                </tbody>
            </table>
</body>
</html>
